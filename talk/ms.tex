\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{palatino}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{multimedia}
\usepackage{hyperref}

\usetheme{Warsaw}
\usecolortheme{crane}

% www.sharelatex.com/learn/Beamer

\title{Computing Entropies with Nested Sampling}
\author{Brendon J. Brewer}
\institute{Department of Statistics\\
The University of Auckland}
\date{\color{blue}\url{https://www.stat.auckland.ac.nz/~brewer/}}

\begin{document}

\frame{\titlepage}


% New slide
\begin{frame}
\frametitle{What is entropy?}

Firstly, I'm talking about information theory, not thermodynamics (though the
two are connected).

\end{frame}


% New slide
\begin{frame}
\frametitle{Shannon entropy}

Consider a discrete probability distribution with probabilities
$\boldsymbol{p} = \{p_i\}$. The Shannon entropy is
\begin{align}
H(\boldsymbol{p}) &= -\sum_i p_i \log p_i
\end{align}

It is a real-valued property of the distribution.

\end{frame}



% New slide
\begin{frame}
\frametitle{Relative entropy}

Consider two discrete probability distributions with probabilities
$\boldsymbol{p} = \{p_i\}$ and $\boldsymbol{q} = \{q_i\}$.
The relative entropy is
\begin{align}
H(\boldsymbol{p}; \boldsymbol{q}) &= -\sum_i p_i \log\left(\frac{p_i}{q_i}\right)
\end{align}
Without the minus sign,
it's the `Kullback-Leibler divergence', and is more fundamental than the
Shannon entropy. With uniform $\boldsymbol{q}$, it reduces to the Shannon
entropy (up to an additive constant).
\end{frame}


% New slide
\begin{frame}
\frametitle{Entropy quantifies uncertainty}
If there are just $N$ equally likely possibilities,
i.e., $p_i = 1/N$, then $H = \log N$. \vspace{0.5em}

\begin{center}
\includegraphics[width=0.6\textwidth]{entropy1.pdf}
\end{center}

\end{frame}


% New slide
\begin{frame}
\frametitle{Entropy quantifies uncertainty}
If there are just $N$ equally likely possibilities,
i.e., $p_i = 1/N$, then $H = \log N$. \vspace{0.5em}

\begin{center}
\includegraphics[width=0.6\textwidth]{entropy2.pdf}
\end{center}

\end{frame}

% New slide
\begin{frame}
\frametitle{Entropy quantifies uncertainty}
If there are just $N$ equally likely possibilities,
i.e., $p_i = 1/N$, then $H = \log N$. \vspace{0.5em}

\begin{center}
\includegraphics[width=0.6\textwidth]{entropy3.pdf}
\end{center}

\end{frame}


% New slide
\begin{frame}
\frametitle{What about densities?}
We get `differential entropy'

\begin{align}
H = - \int_{\textnormal{all }x} f(x) \log f(x) \, dx
\end{align}

This generalises log-volume, as defined {\bf with respect to} $dx$.

\begin{alertblock}{Important}
Differential entropy is {\em coordinate-system dependent}.
\end{alertblock}

\end{frame}






% New slide
\begin{frame}
\frametitle{Some entropies in Bayesian statistics}
Written them in terms of parameters $\theta$ and data
$d$, for Bayesian purposes.

\begin{enumerate}
\item<2-> Entropy of the prior for the parameters $H(\theta)$
\item<3-> Entropy of the conditional prior for the data $H(\theta | d)$
\item<4-> Entropy of the posterior $H(\theta | d)$
\item<5-> Entropy of the prior for the data $H(d)$
\end{enumerate}

\end{frame}


% New slide
\begin{frame}
\frametitle{Some entropies in Bayesian statistics}

\begin{block}{Remark}
{\em Conditional entropies} such as (2) and (3)
are defined using an expectation over the quantity conditioned upon.
\end{block}

\end{frame}

% New slide
\begin{frame}
\frametitle{Connections}

Entropy of the joint prior:
\begin{align}
H(\theta, d) &= H(\theta) + H(d | \theta) \\
                          &= H(d) + H(\theta | d).
\end{align}

Mutual information:
\begin{align}
I(\theta; d) &= H(\theta) - H(\theta | d).
\end{align}
This quantifies dependence --- or more fundamentally,
relevance, or the potential for learning.


{\tiny There are many other ways of expressing $I$.}

\end{frame}


% New slide
\begin{frame}
\frametitle{Pre-data considerations}

We might want to know {\em how relevant the data is to the parameters},
before learning the data. We might want to optimise that quantity for
experimental design. But it's nasty, especially if there's nuisance
parameters.
\end{frame}


% New slide
\begin{frame}
\frametitle{Hard integrals}

E.g.
\begin{align}
H(\theta|d) &= -\int p(d) \int p(\theta|d) \log p(\theta|d) \, d\theta \, dd \\
            &= -\int p(d) \int p(\theta|d)
                    \log\left[\frac{p(\theta)p(d|\theta)}{p(d)}\right] \, d\theta \, dd
\end{align}

But $p(d)$, sitting there inside a logarithm, is already supposed to be
a hard integral (the marginal likelihood / evidence)...

\begin{align}
p(d) &= \int p(\theta) p(d|\theta) \, d\theta
\end{align}

\end{frame}


% New slide
\begin{frame}
\frametitle{Nested Sampling uses {\em constrained priors}}
Instead of using a sequence of distributions
defined by $\pi(\theta)L(\theta)^{1/T}$, use a sequence defined by

\begin{align*}
p(\theta; \ell) &\propto \left\{
    \begin{array}{lr}
        \pi(\theta), & L(\theta) > \ell\\
        0,           & \textnormal{otherwise}.
    \end{array}
\right.
\end{align*}
and have $\ell$ increase at a controlled rate, so that enclosed prior mass
shrinks {\em geometrically}.

\end{frame}




% New slide
\begin{frame}
\frametitle{References I.}

On the connection between Shannon entropy and thermodynamic entropy,
see: \vspace{2em}

{\tiny Jaynes, Edwin T. ``Gibbs vs Boltzmann entropies.''
American Journal of Physics 33, no. 5 (1965): 391-398. \\

Brewer, Brendon J. ``Unscrambling the Second Law of Thermodynamics''
{\color{blue}
  \url{https://quillette.com/2016/01/28/unscrambling-the-second-law-of-thermodynamics}
}
} % tiny

\end{frame}

% New slide
\begin{frame}
\frametitle{Statements and questions}
Consider a problem with three mutually exclusive, exhaustive possibilities
\texttt{a}, \texttt{b}, and \texttt{c}.


\begin{center}
  \includegraphics[width=0.3\textwidth]{lattice1.pdf}
\end{center}

\end{frame}



\end{document}


