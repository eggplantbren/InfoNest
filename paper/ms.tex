%  LaTeX support: latex@mdpi.com
%  In case you need support, please attach any log files that you could have, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================

% LaTeX Class File and Rendering Mode (choose one)
% You will need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================

\documentclass[entropy,article,submit,oneauthor,pdftex,10pt,a4paper]{mdpi}
%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% actuators, administrativesciences, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, appliedsciences, arts, atmosphere, atoms, axioms, batteries, behavioralsciences, beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsciences, buildings, cancers, catalysts, cells, challenges, chemosensors, children, chromatography, climate, coatings, computation, computers, cosmetics, crystals, data, dentistryjournal, diagnostics, diseases, diversity, econometrics, economies, education, electronics, energies, entropy, environments, epigenomes, fermentation, fibers, foods, forests, futureinternet, galaxies, games, gels, genealogy, genes, geosciences, geriatrics, healthcare, horticulturae, humanities, hydrology, informatics, information, inorganics, insects, ijerph, ijfs, ijms, ijns, ijgi, jcdd, jcm, jdb, jfb, jfmk, jimaging, jof, joi, jlpea, jmse, jpm, jrfm, jsan, land, languages, laws, life, lubricants, machines, marinedrugs, materials, mathematics, medicalsciences, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, molbank, molecules, nanomaterials, ncrna, nutrients, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, plants, polymers, processes, proteomes, publications, recycling, religions, remotesensing, resources, risks, robotics, safety, sensors, sinusitis, socialsciences, societies, sports, standards, sustainability, symmetry, systems, technologies, toxics, toxins, universe, vaccines, veterinarysciences, viruses, water
%---------
% article
%---------
% The default type of manuscript is article, but could be replaced by using one of the class options:
% article, review, communication, commentary, bookreview, correction, addendum, editorial, changes, supfile, casereport, comment, conceptpaper, conferencereport, meetingreport, discussion, essay, letter, newbookreceived, opinion, projectreport, reply, retraction, shortnote, technicalnote, creative, datadescriptor (for journal Data), briefreport, hypothesis, interestingimage
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
% Please insert a blank line is before and after all equation and eqnarray environments to ensure proper line numbering when option submit is chosen
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option "pdftex" is for use with pdfLaTeX only. If eps figure are used, use the optioin "dvipdfm", with LaTeX and dvi2pdf only.

%=================================================================
\setcounter{page}{1}
\lastpage{x}
\doinum{}       %\doinum{10.3390/------}
\pubvolume{}    % \pubvolume{xx}
\pubyear{2017}
%\externaleditor{Academic Editor: xx}
\history{In preparation}%\history{Received: xx / Accepted: xx / Published: xx}

\usepackage{dsfont}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{microtype}

\DeclareUnicodeCharacter{00A0}{ }

\definecolor{orange}{rgb}{1, 0.5, 0}
\definecolor{green}{rgb}{0, 0.5, 0}

\renewcommand{\d}{\boldsymbol{d}}
\newcommand{\todo}{\color{orange} \bf}
\newcommand{\query}{\color{green} \bf}
\newcommand{\x}{\boldsymbol{\theta}}
\newcommand{\n}{\boldsymbol{\eta}}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================

% Add packages and commands to include here
% The amsmath, amsthm, amssymb, hyperref, caption, float and color packages are loaded by the MDPI class.
%\usepackage{graphicx}
%\usepackage{subfigure,psfig}

%=================================================================
%% Please use the following mathematics environments:
% \theoremstyle{mdpi}
% \newcounter{thm}
% \setcounter{thm}{0}
% \newcounter{ex}
% \setcounter{ex}{0}
% \newcounter{re}
% \setcounter{re}{0}
%
% \newtheorem{Theorem}[thm]{Theorem}
% \newtheorem{Lemma}[thm]{Lemma}
% \newtheorem{Corollary}[thm]{Corollary}
% \newtheorem{Proposition}[thm]{Proposition}
%
% \theoremstyle{mdpidefinition}
% \newtheorem{Characterization}[thm]{Characterization}
% \newtheorem{Property}[thm]{Property}
% \newtheorem{Problem}[thm]{Problem}
% \newtheorem{Example}[ex]{Example}
% \newtheorem{ExamplesandDefinitions}[ex]{Examples and Definitions}
% \newtheorem{Remark}[re]{Remark}
% \newtheorem{Definition}[thm]{Definition}
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================

% Full title of the paper (Capitalized)
\Title{Calculating Entropies With Nested Sampling}

% Authors (Add full first names)
\Author{Brendon J. Brewer$^{1,}$*}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{
$^{1}$ Department of Statistics, The University of Auckland, Private Bag 92019,
Auckland 1142, New Zealand}

%\contributed{$^\dagger$ These authors contributed equally to this work.}

% Contact information of the corresponding author (Add [2] after \corres if there are more than one corresponding author.)
\corres{{\tt bj.brewer@auckland.ac.nz}}

% Abstract (Do not use inserted blank lines, i.e. \\)
\abstract{}

% Keywords: add 3 to 10 keywords
\keyword{nested sampling; information theory; monte carlo}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{}
%\MSC{}
%\JEL{}

% If this is an expanded version of a conference paper, please cite it here: enter the full citation of your conference paper, and add $^\dagger$ in the end of the title of this article.
%\conference{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For journal Data:

%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}
%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

If an unknown quantity $\x$ has a discrete probability distribution $p(\x)$,
the entropy is
\begin{align}
H_{\x} &= -\sum_{\x} p(\x) \log p(\x)
\end{align}
where the sum is over all of the possible values of $\x$ under consideration.
This probability distribution is implicitly conditional on some prior
information, which is have omitted from the notation.

\section{Entropies in Bayesian inference}

Bayesian inference is the use of probability theory to
describe uncertainty, often about unknown quantities
(``parameters'') $\x$. Some data $\d$, initially unknown but
thought to be relevant to $\x$, is obtained.
The prior information leads the
user to specify a prior distribution $p(\x)$ for the unknown parameters,
along with a ``sampling distribution'' $p(\d | \x)$ describing prior knowledge
of how the data is related to the parameters
(i.e., if the parameters where known, what data would likely be observed?). By the product rule, this yields a {\em joint prior}
\begin{align}
p(\x, \d) &= p(\x)p(\d | \x)
\end{align}
which is the starting point for inference.


The entropy of this joint prior describes the degree to which the question
``what is the value of the pair $(\x, \d)$?'' remains unanswered.

\subsection{The relevance of data}

The degree to which the question ``what is the value of $\x$?'' is
answered by the question ``what is the value of $\d$?'' is given by the
conditional entropy
\begin{align}
H_{\x | \d} &= - \sum_{\d} p(\d) \sum_{\x} p(\x | \d) \log p(\x | \d)
\end{align}
which is the expected value of the entropy of the posterior, averaged over
all possible datasets which might be observed.
This is difficult to compute because the expression
for the posterior distribution
\begin{align}
p(\x | \d) &= \frac{p(\x)p(\d | \x)}{p(\d)}
\end{align}
contains the marginal likelihood $p(\d)$.

It is important to distinguish between $H_{\x | \d}$ and the
entropy of $\x$ given a particular value of $\d$, which might be written
$H_{\x | \d=\d_0}$.
The former measures the degree to which one question answers
another, so is a function of two questions. The latter measures the
degree to which a {\em statement} answered a question, and is a function
of a question and a statement.



\subsection{Mutual information}
The mutual information is another way of describing the relevance of the
data to the parameters. Its definition, and relation to other quantities, is
\begin{align}
I_{\x, \d} &= \sum_{\x} \sum_{\d} p(\x, \d)
                       \log\left[\frac{p(\x, \d)}{p(\x)p(\d)}\right]\\
           &= H_{\x} + H_{\d} - H_{\x, \d}\\
           &= H_{\d} - H_{\d | \x}\\
           &= H_{\x} - H_{\x | \d}.
\end{align}
For present purposes, it is more conveniently written as
\begin{align}
I_{\x, \d} &= \sum_{\x} \sum_{\d} p(\x, \d)
              \log\left[\frac{p(\d | \x)}{p(\d)}\right].
\end{align}
As with the conditional entropy, we see that the difficulty appears in the
form of the log of the marginal likelihood,
$\log p(\d) = \log \sum_{\x} p(\x)p(\d | \x)$.


For experimental design purposes, maximising either $I_{\x, \d}$ or
$H_{\x | \d}$ will produce the same result. {\todo Mention reference priors?}


\section{Nested Sampling}

Nested Sampling \citep[NS][]{skilling2006nested}


\subsection{The sequence of $X$ values}

When $N=1$, the sequence of $X$ values is a Poisson process.

\citet{Walter2015} showed how this view of the NS sequence of points
can be used to construct a
version of NS that produces unbiased estimates of the marginal likelihood.
However, it can also be used to construct an unbiased estimator of
a log-probability, which is more relevant to information theoretic
quantities.

\section{The algorithm}


\subsection{Discrete vs. differential entropies}

Continuous hypothesis spaces are commonly used in practice. Suppose
$\x$ has probability density $f(\x)$. The algorithm actually estimates
\begin{align}
Q &= \int f(\x) \int f(\x')
            \log \left[ P(d(\x', \x) < \epsilon | \x) \right]
                        \, d\x' \, d\x.
\end{align}
If we actually want the differential
entropy
\begin{align}
h_{\x} &= -\int f(x) \log f(x) \, dx
\end{align}
we can use the fact that density equals mass divided
by volume. Assume $\epsilon$ is small, so that
\begin{align}
P(d(\x', \x) < \epsilon | \x)
    &\approx
    f(\x) \int_{d(\x', \x) < \epsilon} \, d\x'\\
    &= f(\x) \, V
\end{align}
where $V$ is the volume of the region where $d(\x', \x) < \epsilon$.

If the distance function $d(\x', \x)$ is chosen to be Cartesian,
the constraint $d(\x', \x) < \epsilon$ corresponds to a ball in the space
of possible $\x'$ values.
The log-volume of a ball of radius $\epsilon$ in $n$ dimensions is
\begin{align}
\log V(\epsilon; n) &= \frac{n}{2}\log \pi
                        - \log \Gamma\left(\frac{n}{2} + 1\right)
                        + n \log \epsilon,
\end{align}
and provides the conversion factor between the differential entropy and
the entropy actually computed from the algorithm.


\section{Example 1: Measuring the mean of a gaussian}

Consider the basic problem of inferring a quantity $\mu$ from
100 observations $\x = \{x_1, ..., x_{100}\}$ whose probability distribution
(conditional on $\mu$) is
\begin{align}
p(x_i | \mu) &\sim \textnormal{Normal}(\mu, 1).
\end{align}
If the prior for $\mu$ is Normal$(0, 10^2)$, then the posterior is
\begin{align}
\mu | \x &\sim \textnormal{Normal}\left(
                                       \frac{1}{100}\sum_{i=1}^{100} x_i,
                                       \left[\frac{10}{\sqrt{10001}}\right]^2
                                       \right).
\end{align}

\begin{align}
H_{\d} &=% 945.42 \pm 1.02 \\ % 1000 mcmc steps
         % 929.26 \pm 1.00 \\ % 2000 mcmc steps
         % 927.40 \pm 0.98 \\ % 3000 mcmc steps
         % 927.26 \pm 1.00 \\ % 4000 mcmc steps
          928.21 \pm 1.00    % 5000 mcmc steps
\end{align}
The log-volume of a 100-dimensional ball of radius $10^{-3}$ is
-782.017. Therefore, the measured differential entropy of
$p(\d)$ is $928.21 - 782.017 = 146.19 \pm 1.00$, consistent with the
true value of $146.499$.


\section{Example 2: Measuring the period of an oscillating signal}


The multimodality of the posterior here raises an interesting issue. Is
the question we really want answered ``what is the value of $T$
{\bf precisely}?'', to which the mutual information relates?
Most practicing scientists would not feel particularly informed to learn
that the vast majority of possibilities had been ruled out, if the
posterior still consisted of several widely separated modes!
Perhaps, in some applications, a more appropriate question is
``what is the value of $T$ to within $\pm$ 10\%''
{\todo or something like that.}


The priors are
\begin{align}
A           &\sim \textnormal{Exponential}(1)  \\
\log_{10} T &\sim \textnormal{Uniform}(-1, 0)  \\
\phi        &\sim \textnormal{Uniform}(0, 2\pi)
\end{align}
$A$ and $\phi$ are nuisance parameters. Let $\x = \log_{10} T$.
The prior standard deviation of the noise, $\sigma$, was set to 0.1.
These results were computed using a tolerance of 0.001 for every quantity.
Results:

% These were produced with a random number seed of 0
% on commit 44525f13edd0d29de43e703db17c594fc0d5045c

\begin{align}                  % (num_particles, mcmc_steps, max_depth, reps)
H_{\x}    &= 6.223 \pm 0.021 \\% (100, 1000, 20.0,  147)
H_{\d}    &= 805.1 \pm 9.7   \\% (100, 1000, 1000.0, 52)
H_{\x,\d} &= 873   \pm 25    \\% (100, 1000, 1200.0, 12)
\end{align}

{\query The multimodality of the posterior in this problem (sometimes) might
affect mixing sufficiently that the NS gets the incorrect answer. Be
careful! And maybe use an example where this is unlikely.}

To convert these to differential entropies, we need to add the appropriate
log-volume to each. These are $-6.215$, $-782.02$, and
$-788.23$ respectively.
%\begin{align}
%H'_{\x}     &= 1.113 \pm 0.025 \\
%H'_{\d}     &= 178.4 \pm 3.7   \\
%H'_{\x, \d} &= 180.4 \pm 3.4
%\end{align}
and therefore a mutual information of {\query a negative value! Oh dear!
Something is wrong. Update: well I found a bug after this, so it might
change. :)}


The computational resources needed to compute these quantities are quite large,
though the algorithm can be parallelised trivially. However, the problem
being solved is not trivial. The mutual information of $\x$ and
$\d$ is
\begin{align}
I_{\x, \d} &= \int p(\x, \d) \log\frac{p(\x, \d)}{p(\x)p(\d)} \, d\x \, d\d.
\end{align}
However, $\x$ is one of several parameters. The others, $A$, and $\phi$,
were implicitly integrated out. Denoting the nuisance parameters by
$\n$, the mutual information that has been calculated is
\begin{align}
I_{\x, \d} &= \int p(\x, \d)
  \log\frac{\int p(\x, \n, \d) \, d\n }
           {p(\x) \int p(\x', \n)p(\d | \x', \n) \, d\x' \, d\n}
  \, d\x \, d\d.
\end{align}
It should not be considered surprising that this is a challenging task.


\section{Example 3: TBD}

\acknowledgments{Acknowledgements}
It is a pleasure to thank the following people for interesting and helpful
conversations about this topic: Ruth Angus (Flatiron Institute),
Ewan Cameron (Oxford), David Hogg (NYU), Kevin Knuth (SUNY Albany),
Iain Murray (Edinburgh). This work was supported by Centre for eResearch
at the University of Auckland.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\authorcontributions{Author Contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\conflictofinterests{Conflicts of Interest}
The authors declare no conflicts of interest.

%=================================================================
% References: Variant A
%=================================================================
% Back Matter (References and Notes)
%----------------------------------------------------------
% Style and layout of the references
%\bibliographystyle{mdpi}
\makeatletter
\renewcommand\@biblabel[1]{#1. }
\makeatother


%=================================================================
% References:  Variant B
%=================================================================
% Use the following option to include external BibTeX files:
\bibliographystyle{mdpi}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\abbreviations{Abbreviations/Nomenclature}
%
%Main text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

