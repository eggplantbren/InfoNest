\usepackage[utf8]{inputenc}

@article{skilling2006nested,
  title={Nested sampling for general Bayesian computation},
  author={John Skilling},
  journal={Bayesian analysis},
  volume={1},
  number={4},
  pages={833--859},
  year={2006},
  publisher={International Society for Bayesian Analysis}
}


@Article{Walter2015,
author="Walter, Cl{\'e}ment",
title="Point process-based Monte Carlo estimation",
journal="Statistics and Computing",
year="2015",
pages="1--18",
abstract="This paper addresses the issue of estimating the expectation of a real-valued random variable of the form                                                                   {\$}{\$}X = g({\backslash}mathbf {\{}U{\}}){\$}{\$}                                                                            X                      =                      g                      (                      U                      )                                                                     where g is a deterministic function and                                                                   {\$}{\$}{\backslash}mathbf {\{}U{\}}{\$}{\$}                                                      U                                                 can be a random finite- or infinite-dimensional vector. Using recent results on rare event simulation, we propose a unified framework for dealing with both probability and mean estimation for such random variables, i.e. linking algorithms such as Tootsie Pop Algorithm or Last Particle Algorithm with nested sampling. Especially, it extends nested sampling as follows: first the random variable X does not need to be bounded any more: it gives the principle of an ideal estimator with an infinite number of terms that is unbiased and always better than a classical Monte Carlo estimator---in particular it has a finite variance as soon as there exists                                                                   {\$}{\$}k {\backslash}in {\backslash}mathbb {\{}R{\}}> 1{\$}{\$}                                                                            k                      ∈                      R                      >                      1                                                                     such that                                                                   {\$}{\$}{\{}{\backslash}text {\{}E{\}}{\}}{\backslash}left[ X^k {\backslash}right] < {\backslash}infty {\$}{\$}                                                                            E                                                                        X                          k                                                                    <                      ∞                                                                    . Moreover we address the issue of nested sampling termination and show that a random truncation of the sum can preserve unbiasedness while increasing the variance only by a factor up to 2 compared to the ideal case. We also build an unbiased estimator with fixed computational budget which supports a Central Limit Theorem and discuss parallel implementation of nested sampling, which can dramatically reduce its running time. Finally we extensively study the case where X is heavy-tailed.",
issn="1573-1375",
doi="10.1007/s11222-015-9617-y",
url="http://dx.doi.org/10.1007/s11222-015-9617-y"
}

