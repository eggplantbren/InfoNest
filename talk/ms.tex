\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{palatino}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multimedia}

\usetheme{Warsaw}
\usecolortheme{crane}

% www.sharelatex.com/learn/Beamer

\title{Computing Entropies with Nested Sampling}
\author{Brendon J. Brewer}
\institute{Department of Statistics\\
The University of Auckland}
\date{{\tt \color{blue} https://www.stat.auckland.ac.nz/\~{ }brewer/}}

\begin{document}

\frame{\titlepage}


% New slide
\begin{frame}
\frametitle{What is entropy?}

Firstly, I'm talking about information theory, not thermodynamics (though the
two are connected).

\end{frame}


% New slide
\begin{frame}
\frametitle{Shannon entropy}

Consider a discrete probability distribution with probabilities
$\boldsymbol{p} = \{p_i\}$. The Shannon entropy is
\begin{align}
H(\boldsymbol{p}) &= -\sum_i p_i \log p_i
\end{align}

It is a real-valued property of the distribution.

\end{frame}



% New slide
\begin{frame}
\frametitle{Relative entropy}

Consider two discrete probability distributions with probabilities
$\boldsymbol{p} = \{p_i\}$ and $\boldsymbol{q} = \{q_i\}$.
The relative entropy is
\begin{align}
H(\boldsymbol{p}; \boldsymbol{q}) &= -\sum_i p_i \log\left(\frac{p_i}{q_i}\right)
\end{align}
Without the minus sign,
it's the `Kullback-Leibler divergence', and is more fundamental than the
Shannon entropy. With uniform $\boldsymbol{q}$, it reduces to the Shannon
entropy (up to an additive constant).
\end{frame}


% New slide
\begin{frame}
\frametitle{Entropy quantifies uncertainty}
If there are just $N$ equally likely possibilities,
i.e., $p_i = 1/N$, then $H = \log N$. \vspace{0.5em}

\begin{center}
\includegraphics[width=0.6\textwidth]{entropy1.pdf}
\end{center}

\end{frame}


% New slide
\begin{frame}
\frametitle{Statements and questions}
Consider a problem with three mutually exclusive, exhaustive possibilities
$a$, $b$, and $c$. The lattice of statements is





\end{frame}



% New slide
\begin{frame}
\frametitle{References I.}

On the connection between Shannon entropy and thermodynamic entropy,
see: \vspace{2em}

{\footnotesize Jaynes, Edwin T. ``Gibbs vs Boltzmann entropies.''
American Journal of Physics 33, no. 5 (1965): 391-398. \\
}

\end{frame}



% New slide
\begin{frame}
\frametitle{References II.}

On the connection between Shannon entropy and thermodynamic entropy,
see: \vspace{2em}

{\footnotesize Jaynes, Edwin T. ``Gibbs vs Boltzmann entropies.''
American Journal of Physics 33, no. 5 (1965): 391-398. \\

Brewer, Brendon J. ``Unscrambling the Second Law of Thermodynamics''
{\color{blue} \tt
http://quillette.com/2016/01/28/unscrambling-the-second-law-of-thermodynamics/}
}

\end{frame}



\end{document}


